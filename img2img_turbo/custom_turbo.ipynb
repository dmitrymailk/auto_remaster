{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/user-name-goes-here/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The config attributes {'shift_factor': 0.0, 'upsample_fn': 'nearest'} were passed to AutoencoderTiny, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.utils.peft_utils import set_weights_and_activate_adapters\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "# from .model import make_1step_sched, my_vae_encoder_fwd, my_vae_decoder_fwd\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoencoderTiny, StableDiffusionPipeline\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import UNet2DModel\n",
    "from diffusers import AutoencoderTiny\n",
    "\n",
    "unet2d_config = {\n",
    "    \"sample_size\": 64,\n",
    "    \"in_channels\": 4,\n",
    "    \"out_channels\": 4,\n",
    "    \"center_input_sample\": False,\n",
    "    \"time_embedding_type\": \"positional\",\n",
    "    \"freq_shift\": 0,\n",
    "    \"flip_sin_to_cos\": True,\n",
    "    \"down_block_types\": (\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "    \"up_block_types\": (\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "    \"block_out_channels\": [320, 640, 1280],\n",
    "    \"layers_per_block\": 1,\n",
    "    \"mid_block_scale_factor\": 1,\n",
    "    \"downsample_padding\": 1,\n",
    "    \"downsample_type\": \"conv\",\n",
    "    \"upsample_type\": \"conv\",\n",
    "    \"dropout\": 0.0,\n",
    "    \"act_fn\": \"silu\",\n",
    "    \"norm_num_groups\": 32,\n",
    "    \"norm_eps\": 1e-05,\n",
    "    \"resnet_time_scale_shift\": \"default\",\n",
    "    \"add_attention\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class Pix2PixLight(torch.nn.Module):\n",
    "    def __init__(self, dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        sched = DDPMScheduler.from_pretrained(\n",
    "            \"stabilityai/sd-turbo\",\n",
    "            subfolder=\"scheduler\",\n",
    "        )\n",
    "        sched.set_timesteps(1, device=\"cuda\")\n",
    "        sched.alphas_cumprod = sched.alphas_cumprod.cuda()\n",
    "        sched.betas = sched.betas.to(dtype).cuda()\n",
    "        sched.alphas = sched.alphas.to(dtype).cuda()\n",
    "        sched.one = sched.one.to(dtype).cuda()\n",
    "        sched.alphas_cumprod = sched.alphas_cumprod.to(dtype).cuda()\n",
    "        self.sched = sched\n",
    "\n",
    "        vae = AutoencoderTiny.from_pretrained(\n",
    "            \"madebyollin/taesd\",\n",
    "            torch_device=\"cuda\",\n",
    "            torch_dtype=dtype,\n",
    "        ).cuda()\n",
    "\n",
    "        vae.decoder.ignore_skip = False\n",
    "        unet = UNet2DModel(**unet2d_config).to(\"cuda\").to(dtype)\n",
    "\n",
    "        # vae.decoder.gamma = 1\n",
    "        self.timesteps = torch.tensor([999], device=\"cuda\").long()\n",
    "        self.unet = unet\n",
    "        self.vae = vae\n",
    "\n",
    "    def set_eval(self):\n",
    "        self.unet.eval()\n",
    "        self.vae.eval()\n",
    "        self.unet.requires_grad_(False)\n",
    "        self.vae.requires_grad_(False)\n",
    "\n",
    "    def set_train(self):\n",
    "        self.unet.train()\n",
    "        self.vae.train()\n",
    "\n",
    "    def forward(self, c_t):\n",
    "        encoded_control = (\n",
    "            self.vae.encode(c_t, False)[0] * self.vae.config.scaling_factor\n",
    "        )\n",
    "        model_pred = self.unet(\n",
    "            encoded_control,\n",
    "            self.timesteps,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "        x_denoised = self.sched.step(\n",
    "            model_pred,\n",
    "            self.timesteps,\n",
    "            encoded_control,\n",
    "            return_dict=False,\n",
    "        )[0]\n",
    "        output_image = (\n",
    "            self.vae.decode(\n",
    "                x_denoised / self.vae.config.scaling_factor,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "        ).clamp(-1, 1)\n",
    "\n",
    "        return output_image\n",
    "\n",
    "    def save_model(self, outf):\n",
    "        self.unet.save_pretrained(outf + \"unet\")\n",
    "        self.vae.save_pretrained(outf + \"vae\")\n",
    "\n",
    "\n",
    "model = Pix2PixLight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.1434836387634277s 0.012705373764038085s per image\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.set_eval()\n",
    "begin = time.time()\n",
    "steps = 90\n",
    "for i in range(steps):\n",
    "    c_t = torch.randn(\n",
    "        [1, 3, 512, 512],\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "    )\n",
    "    model(c_t)\n",
    "total = time.time() - begin\n",
    "print(f\"Inference time: {total}s {total/steps}s per image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfast.compilers.diffusion_pipeline_compiler import (\n",
    "    compile,\n",
    "    compile_unet,\n",
    "    compile_vae,\n",
    ")\n",
    "from sfast.compilers.diffusion_pipeline_compiler import compile, CompilationConfig\n",
    "\n",
    "config = CompilationConfig.Default()\n",
    "config.enable_cuda_graph = True\n",
    "config.enable_triton = True\n",
    "config.enable_xformers = True\n",
    "model.vae = compile_vae(model.vae, config)\n",
    "model.unet = compile_unet(model.unet, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.6635997295379639s 0.0073733303281995986s per image\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "steps = 90\n",
    "for i in range(steps):\n",
    "    c_t = torch.randn(\n",
    "        [1, 3, 512, 512],\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.bfloat16,\n",
    "    )\n",
    "    model(c_t)\n",
    "total = time.time() - begin\n",
    "print(f\"Inference time: {total}s {total/steps}s per image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
