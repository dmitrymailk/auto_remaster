{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d1c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"dim/nfs_pix2pix_1920_1080_v5\"\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    cache_dir=\"/code/dataset/nfs_pix2pix_1920_1080_v5\",\n",
    ")\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6357eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_images_ids = list(range(0, len(dataset), 20))\n",
    "rng = random.Random(42)\n",
    "amount = min(100, len(test_images_ids))\n",
    "selected_ids = rng.sample(test_images_ids, amount)\n",
    "# selected_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7882f",
   "metadata": {},
   "source": [
    "### LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d523c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:05<00:00,  7.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35777404765750087"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lpips\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "resolution = 512\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5],\n",
    "            [0.5],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "loss_fn_vgg = lpips.LPIPS(net=\"vgg\").requires_grad_(False).cuda()\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids):\n",
    "        item_1 = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).cuda()\n",
    "        # item_1 = valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\")).cuda()\n",
    "        item_2 = valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\")).cuda()\n",
    "\n",
    "        d = loss_fn_vgg(item_1, item_2).item()\n",
    "        # print(d)\n",
    "        total_loss += d\n",
    "total_loss / len(selected_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba9149",
   "metadata": {},
   "source": [
    "## SSIM, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ac2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute ssim: 100%|██████████| 43/43 [00:05<00:00,  7.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.55576855), np.float64(0.05032231584813293))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "ssim_preds = []\n",
    "mse_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute ssim\"):\n",
    "        original = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).numpy()\n",
    "        # item_2 = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).numpy()\n",
    "        generated = valid_transforms(\n",
    "            dataset[num][\"edited_image\"].convert(\"RGB\")\n",
    "        ).numpy()\n",
    "        ssim_res = ssim(\n",
    "            original,\n",
    "            generated,\n",
    "            data_range=generated.max() - generated.min(),\n",
    "            channel_axis=0,\n",
    "        )\n",
    "        mse_res = mean_squared_error(original, generated)\n",
    "        ssim_preds.append(ssim_res)\n",
    "        mse_preds.append(mse_res)\n",
    "np.mean(ssim_preds), np.mean(mse_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2625919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute ssim: 100%|██████████| 43/43 [00:04<00:00, 10.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6675837677578593)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import piqa\n",
    "\n",
    "ssim = piqa.SSIM().cuda()\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ssim_preds = []\n",
    "mse_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute ssim\"):\n",
    "        original = (\n",
    "            valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # item_2 = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).numpy()\n",
    "        generated = (\n",
    "            valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        ssim_res = ssim(\n",
    "            original,\n",
    "            generated,\n",
    "        ).item()\n",
    "        ssim_preds.append(ssim_res)\n",
    "\n",
    "np.mean(ssim_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad6ce0",
   "metadata": {},
   "source": [
    "### Dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b79a3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute dists: 100%|██████████| 43/43 [00:29<00:00,  1.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.1629)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import rand\n",
    "from torchmetrics.image.dists import DeepImageStructureAndTextureSimilarity\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dists = DeepImageStructureAndTextureSimilarity().cuda()\n",
    "dists_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute dists\"):\n",
    "        original = (\n",
    "            valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # generated = (\n",
    "        #     valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "        #     .cuda()\n",
    "        #     .unsqueeze(0)\n",
    "        # )\n",
    "        generated = (\n",
    "            valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        dists_res = dists(\n",
    "            generated,\n",
    "            original,\n",
    "        ).item()\n",
    "        dists_preds.append(dists_res)\n",
    "\n",
    "np.round(np.mean(dists_preds), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cfdf8",
   "metadata": {},
   "source": [
    "### psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "094db632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute psnr: 100%|██████████| 43/43 [00:04<00:00,  9.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(19.2376), 19.00299072265625)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "\n",
    "psnr = PeakSignalNoiseRatio(data_range=1.0).cuda()\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "psnr_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute psnr\"):\n",
    "        original = (\n",
    "            valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # generated = (\n",
    "        #     valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "        #     .cuda()\n",
    "        #     .unsqueeze(0)\n",
    "        # )\n",
    "        generated = (\n",
    "            valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        psnr_res = psnr(\n",
    "            generated,\n",
    "            original,\n",
    "        ).item()\n",
    "        psnr.update(generated, original)\n",
    "        psnr_preds.append(psnr_res)\n",
    "\n",
    "np.round(np.mean(psnr_preds), 4), psnr.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d8616",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1af44322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute fid: 100%|██████████| 43/43 [00:03<00:00, 11.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(83.6904)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import rand\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch\n",
    "\n",
    "fid = FrechetInceptionDistance(\n",
    "    feature=2048,\n",
    ").cuda()\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute fid\"):\n",
    "        original = (\n",
    "            torch.tensor(\n",
    "                np.array(valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))),\n",
    "                dtype=torch.uint8,\n",
    "            )\n",
    "            .cuda()\n",
    "            .permute((2, 0, 1))\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # generated = (\n",
    "        #     torch.tensor(np.array(dataset[num][\"input_image\"].convert(\"RGB\")))\n",
    "        #     .cuda()\n",
    "        #     .permute((2, 0, 1))\n",
    "        #     .unsqueeze(0)\n",
    "        # )\n",
    "        generated = (\n",
    "            torch.tensor(\n",
    "                np.array(valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))),\n",
    "                dtype=torch.uint8,\n",
    "            )\n",
    "            .cuda()\n",
    "            .permute((2, 0, 1))\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        fid.update(original, real=True)\n",
    "        fid.update(generated, real=False)\n",
    "    final_fid = fid.compute()\n",
    "\n",
    "np.round(final_fid.item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b26b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LPIPS...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Initializing SSIM...\n",
      "Initializing DISTS...\n",
      "Initializing PSNR...\n",
      "Initializing FID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 43/43 [00:31<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing FID score...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lpips': 0.3578,\n",
       " 'mse': 0.0126,\n",
       " 'ssim': 0.6676,\n",
       " 'dists': 0.1629,\n",
       " 'psnr': 19.2376,\n",
       " 'fid': 83.6904}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import lpips\n",
    "import piqa\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.dists import DeepImageStructureAndTextureSimilarity\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageEvaluator:\n",
    "    def __init__(\n",
    "        self, metrics_list: List[str], device: str = \"cuda\", resolution: int = 512\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ВАЖНО, при использовании усреднении через средства питона, накапливаются ошибки\n",
    "        и результат отличается если сначала все сложить через либу, а затем вызвать метрику compute.\n",
    "        ключевые различия можно обнаружить на метрике psnr и fid\n",
    "        Инициализация класса для оценки качества изображений.\n",
    "\n",
    "        Args:\n",
    "            metrics_list: Список метрик ('lpips', 'mse', 'ssim', 'dists', 'psnr', 'fid').\n",
    "            device: Устройство для вычислений ('cuda' или 'cpu').\n",
    "            resolution: Разрешение для ресайза и кропа изображений.\n",
    "        \"\"\"\n",
    "        self.metrics_list = [m.lower() for m in metrics_list]\n",
    "        self.device = device\n",
    "        self.resolution = resolution\n",
    "\n",
    "        # --- Определение трансформаций ---\n",
    "\n",
    "        # 1. Базовая трансформация: [0, 1] (для SSIM, PSNR, MSE, DISTS)\n",
    "        self.base_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    resolution, interpolation=transforms.InterpolationMode.LANCZOS\n",
    "                ),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 2. Трансформация для LPIPS: [-1, 1]\n",
    "        self.lpips_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    resolution, interpolation=transforms.InterpolationMode.LANCZOS\n",
    "                ),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.5, 0.5, 0.5),\n",
    "                    (0.5, 0.5, 0.5),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 3. Трансформация для FID: [0, 255] uint8 tensor\n",
    "        # FID в torchmetrics ожидает (N, 3, H, W) типа uint8\n",
    "        self.fid_resize_crop = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    resolution, interpolation=transforms.InterpolationMode.LANCZOS\n",
    "                ),\n",
    "                transforms.CenterCrop(resolution),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # --- Инициализация моделей ---\n",
    "        self._init_models()\n",
    "\n",
    "    def _init_models(self):\n",
    "        \"\"\"Загрузка необходимых моделей в память.\"\"\"\n",
    "\n",
    "        if \"lpips\" in self.metrics_list:\n",
    "            print(\"Initializing LPIPS...\")\n",
    "            self.loss_fn_lpips = (\n",
    "                lpips.LPIPS(net=\"vgg\").requires_grad_(False).to(self.device)\n",
    "            )\n",
    "\n",
    "        if \"ssim\" in self.metrics_list:\n",
    "            print(\"Initializing SSIM...\")\n",
    "            self.loss_fn_ssim = piqa.SSIM().to(self.device)\n",
    "\n",
    "        if \"dists\" in self.metrics_list:\n",
    "            print(\"Initializing DISTS...\")\n",
    "            self.loss_fn_dists = DeepImageStructureAndTextureSimilarity().to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "        if \"psnr\" in self.metrics_list:\n",
    "            print(\"Initializing PSNR...\")\n",
    "            self.loss_fn_psnr = PeakSignalNoiseRatio(data_range=1.0).to(self.device)\n",
    "\n",
    "        if \"fid\" in self.metrics_list:\n",
    "            print(\"Initializing FID...\")\n",
    "            self.loss_fn_fid = FrechetInceptionDistance(feature=2048).to(self.device)\n",
    "\n",
    "    def _prepare_fid_tensor(self, pil_img: Image.Image) -> torch.Tensor:\n",
    "        \"\"\"Конвертирует PIL в тензор uint8 (C, H, W) для FID.\"\"\"\n",
    "        img = self.fid_resize_crop(pil_img)\n",
    "        # Convert to numpy (H, W, C) then to tensor (C, H, W)\n",
    "        array = np.array(img)\n",
    "        tensor = torch.tensor(array, dtype=torch.uint8).to(self.device).permute(2, 0, 1)\n",
    "        return tensor.unsqueeze(0)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        originals: List[Image.Image],\n",
    "        generated: List[Image.Image],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Вычисляет метрики для двух списков изображений.\n",
    "\n",
    "        Args:\n",
    "            originals: Список оригинальных PIL изображений.\n",
    "            generated: Список сгенерированных PIL изображений.\n",
    "\n",
    "        Returns:\n",
    "            Словарь с усредненными значениями метрик.\n",
    "        \"\"\"\n",
    "        assert len(originals) == len(\n",
    "            generated\n",
    "        ), \"Списки изображений должны быть одной длины\"\n",
    "\n",
    "        # Хранилище результатов\n",
    "        results = {k: [] for k in self.metrics_list if k != \"fid\"}\n",
    "\n",
    "        # Сброс состояния FID перед новым расчетом\n",
    "        if \"fid\" in self.metrics_list:\n",
    "            self.loss_fn_fid.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for orig_pil, gen_pil in tqdm(\n",
    "                zip(originals, generated), total=len(originals), desc=\"Evaluating\"\n",
    "            ):\n",
    "\n",
    "                # --- Подготовка данных ---\n",
    "\n",
    "                # Для LPIPS (нормализация [-1, 1])\n",
    "                if \"lpips\" in self.metrics_list:\n",
    "                    orig_lpips = (\n",
    "                        self.lpips_transform(orig_pil.convert(\"RGB\"))\n",
    "                        .to(self.device)\n",
    "                        .unsqueeze(0)\n",
    "                    )\n",
    "                    gen_lpips = (\n",
    "                        self.lpips_transform(gen_pil.convert(\"RGB\"))\n",
    "                        .to(self.device)\n",
    "                        .unsqueeze(0)\n",
    "                    )\n",
    "\n",
    "                    val = self.loss_fn_lpips(orig_lpips, gen_lpips).item()\n",
    "                    results[\"lpips\"].append(val)\n",
    "\n",
    "                # Для SSIM, DISTS, PSNR, MSE (диапазон [0, 1])\n",
    "                need_base = any(\n",
    "                    m in self.metrics_list for m in [\"ssim\", \"dists\", \"psnr\", \"mse\"]\n",
    "                )\n",
    "                if need_base:\n",
    "                    orig_base = (\n",
    "                        self.base_transform(orig_pil.convert(\"RGB\"))\n",
    "                        .to(self.device)\n",
    "                        .unsqueeze(0)\n",
    "                    )\n",
    "                    gen_base = (\n",
    "                        self.base_transform(gen_pil.convert(\"RGB\"))\n",
    "                        .to(self.device)\n",
    "                        .unsqueeze(0)\n",
    "                    )\n",
    "\n",
    "                    if \"ssim\" in self.metrics_list:\n",
    "                        # piqa.SSIM ожидает (N, C, H, W) в [0, 1]\n",
    "                        val = self.loss_fn_ssim(orig_base, gen_base).item()\n",
    "                        results[\"ssim\"].append(val)\n",
    "\n",
    "                    if \"dists\" in self.metrics_list:\n",
    "                        # DISTS(preds, target)\n",
    "                        val = self.loss_fn_dists(gen_base, orig_base).item()\n",
    "                        results[\"dists\"].append(val)\n",
    "\n",
    "                    if \"psnr\" in self.metrics_list:\n",
    "                        # PSNR(preds, target)\n",
    "                        val = self.loss_fn_psnr(gen_base, orig_base).item()\n",
    "                        results[\"psnr\"].append(val)\n",
    "\n",
    "                    if \"mse\" in self.metrics_list:\n",
    "                        val = F.mse_loss(gen_base, orig_base).item()\n",
    "                        results[\"mse\"].append(val)\n",
    "\n",
    "                # Для FID (накопление статистики)\n",
    "                if \"fid\" in self.metrics_list:\n",
    "                    orig_fid = self._prepare_fid_tensor(orig_pil.convert(\"RGB\"))\n",
    "                    gen_fid = self._prepare_fid_tensor(gen_pil.convert(\"RGB\"))\n",
    "\n",
    "                    self.loss_fn_fid.update(orig_fid, real=True)\n",
    "                    self.loss_fn_fid.update(gen_fid, real=False)\n",
    "\n",
    "        # --- Агрегация результатов ---\n",
    "        final_metrics = {}\n",
    "\n",
    "        # Среднее для поэлементных метрик\n",
    "        for name, values in results.items():\n",
    "            if values:\n",
    "                final_metrics[name] = float(np.mean(values))\n",
    "\n",
    "        # Вычисление итогового FID\n",
    "        if \"fid\" in self.metrics_list:\n",
    "            print(\"Computing FID score...\")\n",
    "            fid_score = self.loss_fn_fid.compute()\n",
    "            final_metrics[\"fid\"] = float(fid_score.item())\n",
    "\n",
    "        # Округление\n",
    "        for k, v in final_metrics.items():\n",
    "            final_metrics[k] = round(v, 4)\n",
    "\n",
    "        return final_metrics\n",
    "\n",
    "\n",
    "image_evaluator = ImageEvaluator(\n",
    "    metrics_list=[\n",
    "        \"lpips\",\n",
    "        \"mse\",\n",
    "        \"ssim\",\n",
    "        \"dists\",\n",
    "        \"psnr\",\n",
    "        \"fid\",\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    "    resolution=512,\n",
    ")\n",
    "\n",
    "originals = [dataset[num][\"input_image\"] for num in selected_ids]\n",
    "generated = [dataset[num][\"edited_image\"] for num in selected_ids]\n",
    "image_evaluator.evaluate(\n",
    "    originals=originals,\n",
    "    generated=generated,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b480d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"lpips\": 0.3578,\n",
    "    \"mse\": 0.0126,\n",
    "    \"ssim\": 0.6676,\n",
    "    \"dists\": 0.1629,\n",
    "    \"psnr\": 19.2376,\n",
    "    \"fid\": 83.6904,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e44484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LPIPS...\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /opt/conda/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Initializing SSIM...\n",
      "Initializing DISTS...\n",
      "Initializing PSNR...\n",
      "Initializing FID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [00:03<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing FID score...\n",
      "{'lpips': 0.3578, 'mse': 0.0126, 'ssim': 0.6676, 'dists': 0.1629, 'psnr': 19.003, 'fid': 83.7086}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import lpips\n",
    "import piqa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.dists import DeepImageStructureAndTextureSimilarity\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# --- DATASET (Остается прежним, он эффективен) ---\n",
    "\n",
    "\n",
    "# --- EVALUATOR ---\n",
    "class ImageEvaluator:\n",
    "    class EvaluationDataset(Dataset):\n",
    "        def __init__(\n",
    "            self,\n",
    "            originals: List[Image.Image],\n",
    "            generated: List[Image.Image],\n",
    "            metrics_list: List[str],\n",
    "            resolution: int,\n",
    "        ):\n",
    "            self.originals = originals\n",
    "            self.generated = generated\n",
    "            self.metrics_list = metrics_list\n",
    "            self.resolution = resolution\n",
    "\n",
    "            self.common_transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(\n",
    "                        resolution, interpolation=transforms.InterpolationMode.LANCZOS\n",
    "                    ),\n",
    "                    transforms.CenterCrop(resolution),\n",
    "                ]\n",
    "            )\n",
    "            self.to_tensor = transforms.ToTensor()\n",
    "            self.normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.originals)\n",
    "\n",
    "        def _process_image(self, pil_img):\n",
    "            output = {}\n",
    "            img_resized = self.common_transform(pil_img.convert(\"RGB\"))\n",
    "\n",
    "            need_base = any(\n",
    "                m in self.metrics_list for m in [\"ssim\", \"dists\", \"psnr\", \"mse\"]\n",
    "            )\n",
    "            need_lpips = \"lpips\" in self.metrics_list\n",
    "            need_fid = \"fid\" in self.metrics_list\n",
    "\n",
    "            if need_base or need_lpips:\n",
    "                tensor_base = self.to_tensor(img_resized)\n",
    "                if need_base:\n",
    "                    output[\"base\"] = tensor_base\n",
    "                if need_lpips:\n",
    "                    output[\"lpips\"] = self.normalize(tensor_base)\n",
    "\n",
    "            if need_fid:\n",
    "                arr = np.array(img_resized)\n",
    "                tensor_fid = (\n",
    "                    torch.from_numpy(arr).permute(2, 0, 1).to(dtype=torch.uint8)\n",
    "                )\n",
    "                output[\"fid\"] = tensor_fid\n",
    "\n",
    "            return output\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                \"orig\": self._process_image(self.originals[idx]),\n",
    "                \"gen\": self._process_image(self.generated[idx]),\n",
    "            }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics_list: List[str],\n",
    "        device: str = \"cuda\",\n",
    "        resolution: int = 512,\n",
    "        num_workers: int = 4,\n",
    "    ):\n",
    "        self.metrics_list = [m.lower() for m in metrics_list]\n",
    "        self.device = device\n",
    "        self.resolution = resolution\n",
    "        self.num_workers = num_workers\n",
    "        self._init_models()\n",
    "\n",
    "    def _init_models(self):\n",
    "        # 1. LPIPS (нет параметра reduction в init, делаем sum при вызове)\n",
    "        if \"lpips\" in self.metrics_list:\n",
    "            print(\"Initializing LPIPS...\")\n",
    "            self.loss_fn_lpips = (\n",
    "                lpips.LPIPS(net=\"vgg\").requires_grad_(False).to(self.device)\n",
    "            )\n",
    "\n",
    "        # 2. SSIM (piqa поддерживает reduction='sum')\n",
    "        if \"ssim\" in self.metrics_list:\n",
    "            print(\"Initializing SSIM...\")\n",
    "            self.loss_fn_ssim = piqa.SSIM(reduction=\"sum\").to(self.device)\n",
    "\n",
    "        # 3. TorchMetrics (PSNR, DISTS, FID) - они сами умеют накапливать состояние через update()\n",
    "        if \"dists\" in self.metrics_list:\n",
    "            print(\"Initializing DISTS...\")\n",
    "            self.metric_dists = DeepImageStructureAndTextureSimilarity().to(self.device)\n",
    "\n",
    "        if \"psnr\" in self.metrics_list:\n",
    "            print(\"Initializing PSNR...\")\n",
    "            self.metric_psnr = PeakSignalNoiseRatio(data_range=1.0).to(self.device)\n",
    "\n",
    "        if \"fid\" in self.metrics_list:\n",
    "            print(\"Initializing FID...\")\n",
    "            self.metric_fid = FrechetInceptionDistance(feature=2048).to(self.device)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        originals: List[Image.Image],\n",
    "        generated: List[Image.Image],\n",
    "        batch_size: int = 16,\n",
    "    ) -> Dict[str, float]:\n",
    "\n",
    "        n_samples = len(originals)\n",
    "        dataset = self.EvaluationDataset(\n",
    "            originals, generated, self.metrics_list, self.resolution\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        # Сброс метрик torchmetrics\n",
    "        if \"dists\" in self.metrics_list:\n",
    "            self.metric_dists.reset()\n",
    "        if \"psnr\" in self.metrics_list:\n",
    "            self.metric_psnr.reset()\n",
    "        if \"fid\" in self.metrics_list:\n",
    "            self.metric_fid.reset()\n",
    "\n",
    "        # Аккумуляторы для метрик, которые считаем вручную (LPIPS, MSE, SSIM)\n",
    "        manual_sums = {\n",
    "            k: 0.0 for k in [\"lpips\", \"mse\", \"ssim\"] if k in self.metrics_list\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "\n",
    "                # --- LPIPS ---\n",
    "                if \"lpips\" in self.metrics_list:\n",
    "                    orig_l = batch[\"orig\"][\"lpips\"].to(self.device, non_blocking=True)\n",
    "                    gen_l = batch[\"gen\"][\"lpips\"].to(self.device, non_blocking=True)\n",
    "                    # LPIPS возвращает (B, 1, 1, 1), суммируем\n",
    "                    manual_sums[\"lpips\"] += (\n",
    "                        self.loss_fn_lpips(orig_l, gen_l).sum().item()\n",
    "                    )\n",
    "\n",
    "                # --- Base Transforms (SSIM, MSE, DISTS, PSNR) ---\n",
    "                need_base = any(\n",
    "                    m in self.metrics_list for m in [\"ssim\", \"dists\", \"psnr\", \"mse\"]\n",
    "                )\n",
    "                if need_base:\n",
    "                    orig_b = batch[\"orig\"][\"base\"].to(self.device, non_blocking=True)\n",
    "                    gen_b = batch[\"gen\"][\"base\"].to(self.device, non_blocking=True)\n",
    "\n",
    "                    # SSIM (piqa c reduction='sum')\n",
    "                    if \"ssim\" in self.metrics_list:\n",
    "                        manual_sums[\"ssim\"] += self.loss_fn_ssim(orig_b, gen_b).item()\n",
    "\n",
    "                    # MSE (ручной расчет суммы средних ошибок)\n",
    "                    if \"mse\" in self.metrics_list:\n",
    "                        # (gen - orig)^2 -> mean по пикселям -> sum по батчу\n",
    "                        # Это дает нам сумму MSE каждой картинки\n",
    "                        batch_mse_sum = (\n",
    "                            F.mse_loss(gen_b, orig_b, reduction=\"none\")\n",
    "                            .mean(dim=[1, 2, 3])\n",
    "                            .sum()\n",
    "                            .item()\n",
    "                        )\n",
    "                        manual_sums[\"mse\"] += batch_mse_sum\n",
    "\n",
    "                    # DISTS (torchmetrics update)\n",
    "                    if \"dists\" in self.metrics_list:\n",
    "                        self.metric_dists.update(gen_b, orig_b)\n",
    "\n",
    "                    # PSNR (torchmetrics update)\n",
    "                    if \"psnr\" in self.metrics_list:\n",
    "                        self.metric_psnr.update(gen_b, orig_b)\n",
    "\n",
    "                # --- FID (torchmetrics update) ---\n",
    "                if \"fid\" in self.metrics_list:\n",
    "                    orig_f = batch[\"orig\"][\"fid\"].to(self.device, non_blocking=True)\n",
    "                    gen_f = batch[\"gen\"][\"fid\"].to(self.device, non_blocking=True)\n",
    "                    self.metric_fid.update(orig_f, real=True)\n",
    "                    self.metric_fid.update(gen_f, real=False)\n",
    "\n",
    "        # --- Сборка результатов ---\n",
    "        final_metrics = {}\n",
    "\n",
    "        # 1. Метрики с ручным суммированием делим на кол-во сэмплов\n",
    "        for k, v in manual_sums.items():\n",
    "            final_metrics[k] = v / n_samples\n",
    "\n",
    "        # 2. Метрики torchmetrics вычисляют результат сами\n",
    "        if \"dists\" in self.metrics_list:\n",
    "            final_metrics[\"dists\"] = float(self.metric_dists.compute().item())\n",
    "\n",
    "        if \"psnr\" in self.metrics_list:\n",
    "            final_metrics[\"psnr\"] = float(self.metric_psnr.compute().item())\n",
    "\n",
    "        if \"fid\" in self.metrics_list:\n",
    "            print(\"Computing FID score...\")\n",
    "            final_metrics[\"fid\"] = float(self.metric_fid.compute().item())\n",
    "\n",
    "        # Округление\n",
    "        for k, v in final_metrics.items():\n",
    "            final_metrics[k] = round(v, 4)\n",
    "\n",
    "        return final_metrics\n",
    "\n",
    "\n",
    "evaluator = ImageEvaluator(\n",
    "    metrics_list=[\n",
    "        \"lpips\",\n",
    "        \"mse\",\n",
    "        \"ssim\",\n",
    "        \"dists\",\n",
    "        \"psnr\",\n",
    "        \"fid\",\n",
    "    ],\n",
    "    device=\"cuda\",\n",
    "    num_workers=4,  # Используйте 0 для отладки, >0 для скорости (например, cpu_count() / 2)\n",
    ")\n",
    "\n",
    "\n",
    "results = evaluator.evaluate(originals, generated, batch_size=16)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3da8ded0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lpips': 0.3578,\n",
       " 'mse': 0.0126,\n",
       " 'ssim': 0.6676,\n",
       " 'dists': 0.1629,\n",
       " 'psnr': 19.2376,\n",
       " 'fid': 83.6904}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"lpips\": 0.3578,\n",
    "    \"mse\": 0.0126,\n",
    "    \"ssim\": 0.6676,\n",
    "    \"dists\": 0.1629,\n",
    "    \"psnr\": 19.2376,\n",
    "    \"fid\": 83.6904,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31df76d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lpips': 0.3578,\n",
       " 'mse': 0.0126,\n",
       " 'ssim': 0.6676,\n",
       " 'dists': 0.1629,\n",
       " 'psnr': 19.2376,\n",
       " 'fid': 83.6904,\n",
       " 'test': 123}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"lpips\": 0.3578,\n",
    "    \"mse\": 0.0126,\n",
    "    \"ssim\": 0.6676,\n",
    "    \"dists\": 0.1629,\n",
    "    \"psnr\": 19.2376,\n",
    "    \"fid\": 83.6904,\n",
    "    **{\n",
    "\t\t\"test\": 123\n",
    "\t}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
