{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d1c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"dim/nfs_pix2pix_1920_1080_v5\"\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    cache_dir=\"/code/dataset/nfs_pix2pix_1920_1080_v5\",\n",
    ")\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6357eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_images_ids = list(range(0, len(dataset), 20))\n",
    "rng = random.Random(42)\n",
    "amount = min(100, len(test_images_ids))\n",
    "selected_ids = rng.sample(test_images_ids, amount)\n",
    "# selected_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7882f",
   "metadata": {},
   "source": [
    "### LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d523c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/conda/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:05<00:00,  7.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35777404765750087"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lpips\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "resolution = 512\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5],\n",
    "            [0.5],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "loss_fn_vgg = lpips.LPIPS(net=\"vgg\").requires_grad_(False).cuda()\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids):\n",
    "        item_1 = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).cuda()\n",
    "        # item_1 = valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\")).cuda()\n",
    "        item_2 = valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\")).cuda()\n",
    "\n",
    "        d = loss_fn_vgg(item_1, item_2).item()\n",
    "        # print(d)\n",
    "        total_loss += d\n",
    "total_loss / len(selected_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba9149",
   "metadata": {},
   "source": [
    "## SSIM, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ac2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute ssim: 100%|██████████| 43/43 [00:05<00:00,  7.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.55576855), np.float64(0.05032231584813293))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "ssim_preds = []\n",
    "mse_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute ssim\"):\n",
    "        original = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).numpy()\n",
    "        # item_2 = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).numpy()\n",
    "        generated = valid_transforms(\n",
    "            dataset[num][\"edited_image\"].convert(\"RGB\")\n",
    "        ).numpy()\n",
    "        ssim_res = ssim(\n",
    "            original,\n",
    "            generated,\n",
    "            data_range=generated.max() - generated.min(),\n",
    "            channel_axis=0,\n",
    "        )\n",
    "        mse_res = mean_squared_error(original, generated)\n",
    "        ssim_preds.append(ssim_res)\n",
    "        mse_preds.append(mse_res)\n",
    "np.mean(ssim_preds), np.mean(mse_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2625919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute ssim: 100%|██████████| 43/43 [00:04<00:00, 10.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6675837677578593)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import piqa\n",
    "\n",
    "ssim = piqa.SSIM().cuda()\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ssim_preds = []\n",
    "mse_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute ssim\"):\n",
    "        original = (\n",
    "            valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # item_2 = valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\")).numpy()\n",
    "        generated = (\n",
    "            valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        ssim_res = ssim(\n",
    "            original,\n",
    "            generated,\n",
    "        ).item()\n",
    "        ssim_preds.append(ssim_res)\n",
    "\n",
    "np.mean(ssim_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad6ce0",
   "metadata": {},
   "source": [
    "### Dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b79a3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute dists: 100%|██████████| 43/43 [00:29<00:00,  1.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.1629)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import rand\n",
    "from torchmetrics.image.dists import DeepImageStructureAndTextureSimilarity\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dists = DeepImageStructureAndTextureSimilarity().cuda()\n",
    "dists_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute dists\"):\n",
    "        original = (\n",
    "            valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # generated = (\n",
    "        #     valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "        #     .cuda()\n",
    "        #     .unsqueeze(0)\n",
    "        # )\n",
    "        generated = (\n",
    "            valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        dists_res = dists(\n",
    "            generated,\n",
    "            original,\n",
    "        ).item()\n",
    "        dists_preds.append(dists_res)\n",
    "\n",
    "np.round(np.mean(dists_preds), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281cfdf8",
   "metadata": {},
   "source": [
    "### psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094db632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute psnr: 100%|██████████| 43/43 [00:04<00:00, 10.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(18.7176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "\n",
    "psnr = PeakSignalNoiseRatio(data_range=1.0).cuda()\n",
    "\n",
    "\n",
    "psnr_preds = []\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute psnr\"):\n",
    "        original = (\n",
    "            valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # generated = (\n",
    "        #     valid_transforms(dataset[num][\"input_image\"].convert(\"RGB\"))\n",
    "        #     .cuda()\n",
    "        #     .unsqueeze(0)\n",
    "        # )\n",
    "        generated = (\n",
    "            valid_transforms(dataset[num][\"edited_image\"].convert(\"RGB\"))\n",
    "            .cuda()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        psnr_res = psnr(\n",
    "            generated,\n",
    "            original,\n",
    "        ).item()\n",
    "        psnr_preds.append(psnr_res)\n",
    "\n",
    "np.round(np.mean(psnr_res), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d8616",
   "metadata": {},
   "source": [
    "### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1af44322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compute fid: 100%|██████████| 43/43 [00:02<00:00, 17.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5267)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import rand\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch\n",
    "\n",
    "fid = FrechetInceptionDistance(\n",
    "    feature=768,\n",
    ").cuda()\n",
    "\n",
    "valid_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            resolution,\n",
    "            interpolation=transforms.InterpolationMode.LANCZOS,\n",
    "        ),\n",
    "        transforms.CenterCrop(resolution),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for num in tqdm(selected_ids, desc=\"compute fid\"):\n",
    "        original = (\n",
    "            torch.tensor(np.array(dataset[num][\"input_image\"].convert(\"RGB\")))\n",
    "            .cuda()\n",
    "            .permute((2, 0, 1))\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        # generated = (\n",
    "        #     torch.tensor(np.array(dataset[num][\"input_image\"].convert(\"RGB\")))\n",
    "        #     .cuda()\n",
    "        #     .permute((2, 0, 1))\n",
    "        #     .unsqueeze(0)\n",
    "        # )\n",
    "        generated = (\n",
    "            torch.tensor(np.array(dataset[num][\"edited_image\"].convert(\"RGB\")))\n",
    "            .cuda()\n",
    "            .permute((2, 0, 1))\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "        fid.update(original, real=True)\n",
    "        fid.update(generated, real=False)\n",
    "    final_fid = fid.compute()\n",
    "\n",
    "np.round(final_fid.item(), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
