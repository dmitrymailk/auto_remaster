{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416fbf8e-28cb-4a33-85a4-ac04ad63f13c\n"
     ]
    }
   ],
   "source": [
    "# This is an example that uses the websockets api to know when a prompt execution is done\n",
    "# Once the prompt execution is done it downloads the images using the /history endpoint\n",
    "# https://github.com/Nuked88/DreamingAI/blob/main/dreaminAI_websockets_api_example.py\n",
    "import websocket  # NOTE: websocket-client (https://github.com/websocket-client/websocket-client)\n",
    "import uuid\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "server_address = \"127.0.0.1:8188\"\n",
    "client_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "def queue_prompt(prompt):\n",
    "    p = {\"prompt\": prompt, \"client_id\": client_id}\n",
    "    data = json.dumps(p).encode(\"utf-8\")\n",
    "    req = urllib.request.Request(\"http://{}/prompt\".format(server_address), data=data)\n",
    "    return json.loads(urllib.request.urlopen(req).read())\n",
    "\n",
    "\n",
    "def get_image(filename, subfolder, folder_type):\n",
    "    data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "    url_values = urllib.parse.urlencode(data)\n",
    "    with urllib.request.urlopen(\n",
    "        \"http://{}/view?{}\".format(server_address, url_values)\n",
    "    ) as response:\n",
    "        return response.read()\n",
    "\n",
    "\n",
    "def get_history(prompt_id):\n",
    "    with urllib.request.urlopen(\n",
    "        \"http://{}/history/{}\".format(server_address, prompt_id)\n",
    "    ) as response:\n",
    "        return json.loads(response.read())\n",
    "\n",
    "\n",
    "def get_images(ws, prompt):\n",
    "    prompt_id = queue_prompt(prompt)[\"prompt_id\"]\n",
    "    output_images = {}\n",
    "    while True:\n",
    "        out = ws.recv()\n",
    "        if isinstance(out, str):\n",
    "            message = json.loads(out)\n",
    "            if message[\"type\"] == \"executing\":\n",
    "                data = message[\"data\"]\n",
    "                if data[\"node\"] is None and data[\"prompt_id\"] == prompt_id:\n",
    "                    break  # Execution is done\n",
    "        else:\n",
    "            continue  # previews are binary data\n",
    "\n",
    "    history = get_history(prompt_id)[prompt_id]\n",
    "    for o in history[\"outputs\"]:\n",
    "        for node_id in history[\"outputs\"]:\n",
    "            node_output = history[\"outputs\"][node_id]\n",
    "            if \"images\" in node_output:\n",
    "                images_output = []\n",
    "                for image in node_output[\"images\"]:\n",
    "                    image_data = get_image(\n",
    "                        image[\"filename\"], image[\"subfolder\"], image[\"type\"]\n",
    "                    )\n",
    "                    images_output.append(image_data)\n",
    "            output_images[node_id] = images_output\n",
    "\n",
    "    return output_images\n",
    "\n",
    "\n",
    "# workflow_path = \"showcases/showcase_9/nfs_canny_normal_map_sdxl_api_1.json\"\n",
    "workflow_path = \"showcases/showcase_9/nfs_canny_normal_map_sdxl_batch_list_api.json\"\n",
    "# upload an image\n",
    "with open(workflow_path, \"r\") as f:\n",
    "    workflow = json.load(f)\n",
    "\n",
    "workflow[\"240\"][\"inputs\"][\"steps\"] = 5\n",
    "workflow[\"253\"][\"inputs\"][\"image_load_cap\"] = 4\n",
    "# start\n",
    "workflow[\"256\"][\"inputs\"][\"int\"] = 170\n",
    "workflow[\"258\"][\"inputs\"][\"filename_prefix\"] = f\"nfs_4screens_6_sdxl_{client_id}\"\n",
    "\n",
    "print(client_id)\n",
    "ws = websocket.WebSocket()\n",
    "ws.connect(\"ws://{}/ws?clientId={}\".format(server_address, client_id))\n",
    "images = get_images(ws, workflow)\n",
    "\n",
    "# Commented out code to display the output images:\n",
    "images_list = []\n",
    "for node_id in images:\n",
    "    for image_data in images[node_id]:\n",
    "        from PIL import Image\n",
    "        import io\n",
    "\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        # image.show()\n",
    "        images_list.append(image)\n",
    "        # save image\n",
    "        # image.save(f\"{node_id}-{seed}.png\")\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from more_itertools import chunked\n",
    "\n",
    "\n",
    "dataset = \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6\"\n",
    "target_save_path_1 = \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6_test\"\n",
    "original_images = sorted(list(Path(dataset).glob(\"*.png\")))\n",
    "target_images = sorted(list(Path(target_save_path_1).glob(\"*.png\")))\n",
    "\n",
    "total_parts = 4\n",
    "original_images_parts = list(\n",
    "    chunked(\n",
    "        original_images,\n",
    "        len(original_images) // total_parts + 1,\n",
    "    )\n",
    ")\n",
    "\n",
    "part_num = 0\n",
    "original_images_part = original_images_parts[part_num]\n",
    "target_images_names = set([item.stem for item in target_images])\n",
    "images_to_process = [\n",
    "    item for item in original_images_part if not item.stem in target_images_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_num = 0\n",
    "original_images_part = original_images_parts[part_num]\n",
    "target_images_names = set([item.stem for item in target_images])\n",
    "images_to_process = [\n",
    "    item for item in original_images_part if not item.stem in target_images_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000007.png')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_to_process[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000000001', '000000002', '000000003', '000000004', '000000005', '000000006'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_images_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000007.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000008.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000009.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000010.png\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([str(item) for item in images_to_process[:4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ComfyUI Worker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000003.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000004.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000005.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000006.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000007.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000008.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000009.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000010.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000011.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000012.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000013.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000014.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000015.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000016.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000017.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000018.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000019.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000020.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000021.png\n",
      "/code/comfyui_sandbox/video_renders/render_nfs_4screens_6/000000022.png\n",
      "67d8822b-f303-4875-af14-ef0ef3725059\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from more_itertools import chunked\n",
    "import websocket\n",
    "import uuid\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import requests\n",
    "import io\n",
    "\n",
    "\n",
    "class ComfyUIImageAPI:\n",
    "    def __init__(\n",
    "        self,\n",
    "        server_address=\"127.0.0.1:8188\",\n",
    "        workflow_path=\"\",\n",
    "    ):\n",
    "        self.server_address = server_address\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "        self.workflow_path = workflow_path\n",
    "\n",
    "    def queue_prompt(\n",
    "        self,\n",
    "        prompt,\n",
    "    ):\n",
    "        p = {\"prompt\": prompt, \"client_id\": self.client_id}\n",
    "        data = json.dumps(p).encode(\"utf-8\")\n",
    "        req = urllib.request.Request(\n",
    "            \"http://{}/prompt\".format(self.server_address), data=data\n",
    "        )\n",
    "        return json.loads(urllib.request.urlopen(req).read())\n",
    "\n",
    "    def get_image(\n",
    "        self,\n",
    "        filename,\n",
    "        subfolder,\n",
    "        folder_type,\n",
    "    ):\n",
    "        data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "        url_values = urllib.parse.urlencode(data)\n",
    "        with urllib.request.urlopen(\n",
    "            \"http://{}/view?{}\".format(self.server_address, url_values)\n",
    "        ) as response:\n",
    "            return response.read()\n",
    "\n",
    "    def get_history(\n",
    "        self,\n",
    "        prompt_id,\n",
    "    ):\n",
    "        with urllib.request.urlopen(\n",
    "            \"http://{}/history/{}\".format(self.server_address, prompt_id)\n",
    "        ) as response:\n",
    "            return json.loads(response.read())\n",
    "\n",
    "    def get_images(\n",
    "        self,\n",
    "        ws,\n",
    "        prompt,\n",
    "    ):\n",
    "        prompt_id = self.queue_prompt(prompt)[\"prompt_id\"]\n",
    "        output_images = {}\n",
    "        while True:\n",
    "            out = ws.recv()\n",
    "            if isinstance(out, str):\n",
    "                message = json.loads(out)\n",
    "                if message[\"type\"] == \"executing\":\n",
    "                    data = message[\"data\"]\n",
    "                    if data[\"node\"] is None and data[\"prompt_id\"] == prompt_id:\n",
    "                        break  # Execution is done\n",
    "            else:\n",
    "                continue  # previews are binary data\n",
    "\n",
    "        history = self.get_history(prompt_id)[prompt_id]\n",
    "        for o in history[\"outputs\"]:\n",
    "            for node_id in history[\"outputs\"]:\n",
    "                node_output = history[\"outputs\"][node_id]\n",
    "                images_output = []\n",
    "                if \"images\" in node_output:\n",
    "                    for image in node_output[\"images\"]:\n",
    "                        image_data = self.get_image(\n",
    "                            image[\"filename\"], image[\"subfolder\"], image[\"type\"]\n",
    "                        )\n",
    "                        images_output.append(image_data)\n",
    "                output_images[node_id] = images_output\n",
    "\n",
    "        return output_images\n",
    "\n",
    "    def process_image_folder(\n",
    "        self,\n",
    "        original_images_path=\"\",\n",
    "        batch_size=4,\n",
    "        target_save_path=\"\",\n",
    "        total_parts=4,\n",
    "        part_num=0,\n",
    "    ):\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "\n",
    "        original_images = sorted(list(Path(original_images_path).glob(\"*.png\")))\n",
    "        target_images = sorted(list(Path(target_save_path).glob(\"*.png\")))\n",
    "\n",
    "        # total_parts = 4\n",
    "        original_images_parts = list(\n",
    "            chunked(\n",
    "                original_images,\n",
    "                len(original_images) // total_parts + 1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # part_num = 0\n",
    "        original_images_part = original_images_parts[part_num]\n",
    "        target_images_names = set([item.stem for item in target_images])\n",
    "        images_to_process = [\n",
    "            item\n",
    "            for item in original_images_part\n",
    "            if not item.stem in target_images_names\n",
    "        ][:batch_size]\n",
    "        if len(images_to_process) == 0:\n",
    "            return \"END\"\n",
    "\n",
    "        with open(self.workflow_path, \"r\") as f:\n",
    "            workflow = json.load(f)\n",
    "\n",
    "        # diffustion steps\n",
    "        # workflow[\"240\"][\"inputs\"][\"steps\"] = 5\n",
    "        workflow[\"201\"][\"inputs\"][\"batch\"] = True\n",
    "        # input images\n",
    "        workflow[\"257\"][\"inputs\"][\"paths\"] = \"\\n\".join(\n",
    "            [str(item) for item in images_to_process]\n",
    "        )\n",
    "        print(workflow[\"257\"][\"inputs\"][\"paths\"])\n",
    "        # save prefix\n",
    "        save_prefix = f\"nfs_4screens_6_sdxl_{self.client_id}\"\n",
    "        workflow[\"258\"][\"inputs\"][\"filename_prefix\"] = save_prefix\n",
    "\n",
    "        print(self.client_id)\n",
    "        ws = websocket.WebSocket()\n",
    "        ws.connect(\n",
    "            \"ws://{}/ws?clientId={}\".format(\n",
    "                self.server_address,\n",
    "                self.client_id,\n",
    "            )\n",
    "        )\n",
    "        images = self.get_images(ws, workflow)\n",
    "\n",
    "        for node_id in images:\n",
    "            for image_data, image_original_path in zip(\n",
    "                images[node_id], images_to_process\n",
    "            ):\n",
    "                image = Image.open(io.BytesIO(image_data))\n",
    "                image.save(f\"{target_save_path}/{image_original_path.stem}.png\")\n",
    "\n",
    "        # clean output images from comfyui\n",
    "        os.system(f\"rm /code/ComfyUI/output/{save_prefix}*.png\")\n",
    "\n",
    "\n",
    "comfy_images_process = ComfyUIImageAPI(\n",
    "    server_address=\"127.0.0.1:8188\",\n",
    "    workflow_path=\"showcases/showcase_9/nfs_canny_normal_map_sdxl_batch_list_api.json\",\n",
    ")\n",
    "dataset = \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6\"\n",
    "target_save_path_1 = \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6_sdxl_1\"\n",
    "comfy_images_process.process_image_folder(\n",
    "    original_images_path=dataset,\n",
    "    batch_size=20,\n",
    "    target_save_path=target_save_path_1,\n",
    "    total_parts=4,\n",
    "    part_num=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Upscale v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install websocket-client==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6165907c6fe4f77a4f76143d6759d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe19336be33495f8c4e842c3a2964dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5392c4ca27f443fb41680baa1a359a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder created at: /tmp/tmp8jagvd1w\n",
      "/tmp/tmp8jagvd1w/0000000.png\n",
      "009ac592-cbb7-4fc5-bd27-182cfe304aed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from more_itertools import chunked\n",
    "import websocket\n",
    "import uuid\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import requests\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "class ComfyUIImageAPIUpscaleV1:\n",
    "    def __init__(\n",
    "        self,\n",
    "        server_address=\"127.0.0.1:8188\",\n",
    "        workflow_path=\"\",\n",
    "    ):\n",
    "        self.server_address = server_address\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "        self.workflow_path = workflow_path\n",
    "\n",
    "    def queue_prompt(\n",
    "        self,\n",
    "        prompt,\n",
    "    ):\n",
    "        p = {\"prompt\": prompt, \"client_id\": self.client_id}\n",
    "        data = json.dumps(p).encode(\"utf-8\")\n",
    "        req = urllib.request.Request(\n",
    "            \"http://{}/prompt\".format(self.server_address), data=data\n",
    "        )\n",
    "        return json.loads(urllib.request.urlopen(req).read())\n",
    "\n",
    "    def get_image(\n",
    "        self,\n",
    "        filename,\n",
    "        subfolder,\n",
    "        folder_type,\n",
    "    ):\n",
    "        data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "        url_values = urllib.parse.urlencode(data)\n",
    "        with urllib.request.urlopen(\n",
    "            \"http://{}/view?{}\".format(self.server_address, url_values)\n",
    "        ) as response:\n",
    "            return response.read()\n",
    "\n",
    "    def get_history(\n",
    "        self,\n",
    "        prompt_id,\n",
    "    ):\n",
    "        with urllib.request.urlopen(\n",
    "            \"http://{}/history/{}\".format(self.server_address, prompt_id)\n",
    "        ) as response:\n",
    "            return json.loads(response.read())\n",
    "\n",
    "    def get_images(\n",
    "        self,\n",
    "        ws,\n",
    "        prompt,\n",
    "    ):\n",
    "        prompt_id = self.queue_prompt(prompt)[\"prompt_id\"]\n",
    "        output_images = {}\n",
    "        while True:\n",
    "            out = ws.recv()\n",
    "            if isinstance(out, str):\n",
    "                message = json.loads(out)\n",
    "                if message[\"type\"] == \"executing\":\n",
    "                    data = message[\"data\"]\n",
    "                    if data[\"node\"] is None and data[\"prompt_id\"] == prompt_id:\n",
    "                        break  # Execution is done\n",
    "            else:\n",
    "                continue  # previews are binary data\n",
    "\n",
    "        history = self.get_history(prompt_id)[prompt_id]\n",
    "        for o in history[\"outputs\"]:\n",
    "            for node_id in history[\"outputs\"]:\n",
    "                node_output = history[\"outputs\"][node_id]\n",
    "                images_output = []\n",
    "                if \"images\" in node_output:\n",
    "                    for image in node_output[\"images\"]:\n",
    "                        image_data = self.get_image(\n",
    "                            image[\"filename\"], image[\"subfolder\"], image[\"type\"]\n",
    "                        )\n",
    "                        images_output.append(image_data)\n",
    "                output_images[node_id] = images_output\n",
    "\n",
    "        return output_images\n",
    "\n",
    "    def process_image_folder(\n",
    "        self,\n",
    "        dataset: list = None,\n",
    "        batch_size=4,\n",
    "        target_save_path_1=\"\",\n",
    "        target_save_path_2=\"\",\n",
    "        total_parts=4,\n",
    "        part_num=0,\n",
    "    ):\n",
    "        os.system(f\"mkdir -p {target_save_path_1}\")\n",
    "        os.system(f\"mkdir -p {target_save_path_2}\")\n",
    "\n",
    "        # генерируем случайный номер для исполнителя\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "\n",
    "        # original_images = sorted(list(Path(original_images_path).glob(\"*.png\")))\n",
    "        original_images = dataset\n",
    "        target_images = sorted(list(Path(target_save_path_1).glob(\"*.png\")))\n",
    "\n",
    "        # разбиваем на части\n",
    "        # total_parts = 4\n",
    "        original_images_parts = list(\n",
    "            chunked(\n",
    "                [\"{:07d}\".format(i) for i in range(len(original_images))],\n",
    "                len(original_images) // total_parts + 1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # part_num = 0\n",
    "        # берем в текущем процессе только нужную часть\n",
    "        original_images_part = original_images_parts[part_num]\n",
    "        # составляем уникальный список имен чтобы с ними больше не работать\n",
    "        target_images_names = set([item.stem for item in target_images])\n",
    "        # фильтруем\n",
    "        images_to_process = [\n",
    "            item for item in original_images_part if not item in target_images_names\n",
    "        ][:batch_size]\n",
    "        if len(images_to_process) == 0:\n",
    "            return \"END\"\n",
    "\n",
    "        with open(self.workflow_path, \"r\") as f:\n",
    "            workflow = json.load(f)\n",
    "\n",
    "        # diffustion steps\n",
    "        # workflow[\"240\"][\"inputs\"][\"steps\"] = 5\n",
    "        # workflow[\"201\"][\"inputs\"][\"batch\"] = True\n",
    "        # input images\n",
    "        # создаем временную папку чтобы данные картинки удалились после\n",
    "        # обработки\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            print(f\"Temporary folder created at: {temp_dir}\")\n",
    "            for im_num in images_to_process:\n",
    "                dataset[int(im_num)][\"edited_image\"].save(f\"{temp_dir}/{im_num}.png\")\n",
    "\n",
    "            images_to_process = [\n",
    "                f\"{temp_dir}/{im_num}.png\" for im_num in images_to_process\n",
    "            ]\n",
    "\n",
    "            # создаем батч\n",
    "            workflow[\"268\"][\"inputs\"][\"paths\"] = \"\\n\".join(\n",
    "                # [str(item) for item in images_to_process]\n",
    "                [item for item in images_to_process]\n",
    "            )\n",
    "            print(workflow[\"268\"][\"inputs\"][\"paths\"])\n",
    "            # save prefix\n",
    "            # save_prefix = f\"nfs_4screens_5_sdxl_{self.client_id}\"\n",
    "            # сохраняем\n",
    "            save_prefix = f\"nfs_4screens_6_sdxl_{self.client_id}\"\n",
    "            workflow[\"261\"][\"inputs\"][\"filename_prefix\"] = save_prefix\n",
    "            workflow[\"252\"][\"inputs\"][\"filename_prefix\"] = save_prefix + \"v2\"\n",
    "\n",
    "            print(self.client_id)\n",
    "            ws = websocket.WebSocket()\n",
    "            ws.connect(\n",
    "                \"ws://{}/ws?clientId={}\".format(\n",
    "                    self.server_address,\n",
    "                    self.client_id,\n",
    "                )\n",
    "            )\n",
    "            # получаем изображения\n",
    "            images = self.get_images(ws, workflow)\n",
    "            for node_id in images:\n",
    "                for image_data, image_original_path in zip(\n",
    "                    images[node_id],\n",
    "                    images_to_process,\n",
    "                ):\n",
    "\n",
    "                    image = Image.open(io.BytesIO(image_data))\n",
    "                    if int(node_id) == 272:\n",
    "                        image.save(\n",
    "                            # f\"{target_save_path_1}/{image_original_path.stem}.png\"\n",
    "                            f\"{target_save_path_1}/{Path(image_original_path).stem}.png\"\n",
    "                        )\n",
    "                    if int(node_id) == 270:\n",
    "                        image.save(\n",
    "                            f\"{target_save_path_2}/{Path(image_original_path).stem}.png\"\n",
    "                        )\n",
    "\n",
    "            # clean output images from comfyui\n",
    "            # os.system(f\"rm /code/ComfyUI/output/{save_prefix}*.png\")\n",
    "            # os.system(f\"rm /code/ComfyUI/output/{save_prefix+'v2'}*.png\")\n",
    "\n",
    "\n",
    "comfy_images_process = ComfyUIImageAPIUpscaleV1(\n",
    "    server_address=\"127.0.0.1:8188\",\n",
    "    workflow_path=\"/code/showcases/showcase_10/workflow_ultimate_upscale_simple_nfs_mix_api_v3.json\",\n",
    ")\n",
    "# original_images_path = (\n",
    "#     \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_5_sdxl_1\"\n",
    "# )\n",
    "target_save_path_1 = (\n",
    "    \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6_sdxl_1_upscale_1x\"\n",
    ")\n",
    "target_save_path_2 = (\n",
    "    \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6_sdxl_1_upscale_2x\"\n",
    ")\n",
    "\n",
    "# port = 8188\n",
    "# port = 1337\n",
    "# port = 1338\n",
    "# port = 1339\n",
    "# comfy_images_process = ComfyUIImageAPI(\n",
    "#     server_address=f\"127.0.0.1:{port}\",\n",
    "#     # workflow_path=\"showcases/showcase_9/nfs_canny_normal_map_sdxl_batch_list_api.json\",\n",
    "#     workflow_path=\"showcases/showcase_9/nfs_canny_normal_map_sdxl_batch_list_api_v2.json\",\n",
    "# )\n",
    "# original_images_path = \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6\"\n",
    "# target_save_path = \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6_sdxl_1\"\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     result = \"\"\n",
    "#     while result != \"END\":\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset_name = \"dim/nfs_pix2pix_1920_1080_v5\"\n",
    "dataset_name = \"dim/nfs_pix2pix_1920_1080_v6\"\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    # cache_dir=\"/code/dataset/nfs_pix2pix_1920_1080_v5\",\n",
    "    cache_dir=\"/code/dataset/nfs_pix2pix_1920_1080_v6\",\n",
    ")\n",
    "dataset = dataset[\"train\"]\n",
    "\n",
    "result = comfy_images_process.process_image_folder(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    target_save_path_1=target_save_path_1,\n",
    "    target_save_path_2=target_save_path_2,\n",
    "    total_parts=50,\n",
    "    part_num=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(14 + 4 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"0000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process_folder'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"comfyui_sandbox/process_folder.py\").stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary folder created at: /tmp/tmpq2hybjoe\n",
      "Folder exists? False\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# The folder is created here\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    print(f\"Temporary folder created at: {temp_dir}\")\n",
    "\n",
    "    # You can now create files inside it\n",
    "    file_path = os.path.join(temp_dir, \"test_file.txt\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"Hello World\")\n",
    "\n",
    "# As soon as you un-indent here, the folder 'temp_dir' is DELETED automatically.\n",
    "print(f\"Folder exists? {os.path.exists(temp_dir)}\")  # Will print False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpq2hybjoe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000025369.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "images = sorted(\n",
    "    os.listdir(\"comfyui_sandbox/video_renders/render_nfs_4screens_6_sdxl_1_part_2\")\n",
    ")\n",
    "images[8000 + 368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3418"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000 - 164 + 500 + 80 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video double upscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_path = (\n",
    "    \"/code/showcases/showcase_11/wan_vace_seedvr2_wan2.2_double_upscale_nfs_api.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a63e73e693646abb613f02aad34adab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8afe437584245729585c8f8b9e5dab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea1bb6019749a3922e3e1ac6fcf47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_frame 0\n",
      "Temporary folder created at: /tmp/tmp9xw68l72\n",
      "Temporary folder created at: /tmp/tmps1tchmxp\n",
      "132bf5a0-04c4-4c17-8b28-13d0e2948ec3\n",
      "['0000000', '0000001', '0000002', '0000003', '0000004', '0000005', '0000006', '0000007', '0000008', '0000009', '0000010', '0000011', '0000012', '0000013', '0000014', '0000015', '0000016', '0000017', '0000018', '0000019', '0000020', '0000021', '0000022', '0000023', '0000024', '0000025', '0000026', '0000027', '0000028', '0000029', '0000030', '0000031', '0000032', '0000033', '0000034', '0000035', '0000036', '0000037', '0000038', '0000039', '0000040', '0000041', '0000042', '0000043', '0000044', '0000045', '0000046', '0000047', '0000048', '0000049', '0000050', '0000051', '0000052', '0000053', '0000054', '0000055', '0000056', '0000057', '0000058', '0000059', '0000060', '0000061', '0000062', '0000063', '0000064', '0000065', '0000066', '0000067', '0000068', '0000069', '0000070', '0000071', '0000072', '0000073', '0000074', '0000075', '0000076', '0000077', '0000078', '0000079', '0000080', '0000081', '0000082', '0000083', '0000084', '0000085', '0000086', '0000087', '0000088', '0000089', '0000090', '0000091', '0000092', '0000093', '0000094', '0000095', '0000096', '0000097', '0000098', '0000099', '0000100', '0000101', '0000102', '0000103', '0000104', '0000105', '0000106', '0000107', '0000108', '0000109', '0000110', '0000111', '0000112', '0000113', '0000114', '0000115', '0000116', '0000117', '0000118', '0000119', '0000120', '0000121', '0000122', '0000123', '0000124', '0000125', '0000126', '0000127', '0000128', '0000129', '0000130', '0000131', '0000132', '0000133', '0000134', '0000135', '0000136', '0000137', '0000138', '0000139', '0000140', '0000141', '0000142', '0000143', '0000144', '0000145', '0000146', '0000147', '0000148', '0000149', '0000150', '0000151', '0000152', '0000153', '0000154', '0000155', '0000156', '0000157', '0000158', '0000159', '0000160', '0000161', '0000162', '0000163', '0000164', '0000165', '0000166', '0000167', '0000168', '0000169', '0000170', '0000171', '0000172', '0000173', '0000174', '0000175', '0000176', '0000177', '0000178', '0000179', '0000180', '0000181', '0000182', '0000183', '0000184', '0000185', '0000186', '0000187', '0000188', '0000189', '0000190', '0000191', '0000192', '0000193', '0000194', '0000195', '0000196', '0000197', '0000198', '0000199', '0000200', '0000201', '0000202', '0000203', '0000204', '0000205', '0000206', '0000207', '0000208', '0000209', '0000210', '0000211', '0000212', '0000213', '0000214', '0000215', '0000216', '0000217', '0000218', '0000219', '0000220', '0000221', '0000222', '0000223', '0000224', '0000225', '0000226', '0000227', '0000228', '0000229', '0000230', '0000231', '0000232', '0000233', '0000234', '0000235', '0000236', '0000237', '0000238', '0000239', '0000240', '0000241', '0000242', '0000243', '0000244', '0000245', '0000246', '0000247', '0000248', '0000249']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from more_itertools import chunked\n",
    "import websocket\n",
    "import uuid\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import requests\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "class ComfyUIImageAPIUpscaleV2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        server_address=\"127.0.0.1:8188\",\n",
    "        workflow_path=\"\",\n",
    "    ):\n",
    "        self.server_address = server_address\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "        self.workflow_path = workflow_path\n",
    "\n",
    "    def queue_prompt(\n",
    "        self,\n",
    "        prompt,\n",
    "    ):\n",
    "        p = {\"prompt\": prompt, \"client_id\": self.client_id}\n",
    "        data = json.dumps(p).encode(\"utf-8\")\n",
    "        req = urllib.request.Request(\n",
    "            \"http://{}/prompt\".format(self.server_address), data=data\n",
    "        )\n",
    "        return json.loads(urllib.request.urlopen(req).read())\n",
    "\n",
    "    def get_image(\n",
    "        self,\n",
    "        filename,\n",
    "        subfolder,\n",
    "        folder_type,\n",
    "    ):\n",
    "        data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n",
    "        url_values = urllib.parse.urlencode(data)\n",
    "        with urllib.request.urlopen(\n",
    "            \"http://{}/view?{}\".format(self.server_address, url_values)\n",
    "        ) as response:\n",
    "            return response.read()\n",
    "\n",
    "    def get_history(\n",
    "        self,\n",
    "        prompt_id,\n",
    "    ):\n",
    "        with urllib.request.urlopen(\n",
    "            \"http://{}/history/{}\".format(self.server_address, prompt_id)\n",
    "        ) as response:\n",
    "            return json.loads(response.read())\n",
    "\n",
    "    def get_images(\n",
    "        self,\n",
    "        ws,\n",
    "        prompt,\n",
    "    ):\n",
    "        prompt_id = self.queue_prompt(prompt)[\"prompt_id\"]\n",
    "        output_images = {}\n",
    "        while True:\n",
    "            out = ws.recv()\n",
    "            if isinstance(out, str):\n",
    "                message = json.loads(out)\n",
    "                if message[\"type\"] == \"executing\":\n",
    "                    data = message[\"data\"]\n",
    "                    if data[\"node\"] is None and data[\"prompt_id\"] == prompt_id:\n",
    "                        break  # Execution is done\n",
    "            else:\n",
    "                continue  # previews are binary data\n",
    "\n",
    "        history = self.get_history(prompt_id)[prompt_id]\n",
    "        for o in history[\"outputs\"]:\n",
    "            for node_id in history[\"outputs\"]:\n",
    "                node_output = history[\"outputs\"][node_id]\n",
    "                images_output = []\n",
    "                if \"images\" in node_output:\n",
    "                    for image in node_output[\"images\"]:\n",
    "                        image_data = self.get_image(\n",
    "                            image[\"filename\"], image[\"subfolder\"], image[\"type\"]\n",
    "                        )\n",
    "                        images_output.append(image_data)\n",
    "                output_images[node_id] = images_output\n",
    "\n",
    "        return output_images\n",
    "\n",
    "    def process_image_folder(\n",
    "        self,\n",
    "        dataset: list = None,\n",
    "        input_frames=250,\n",
    "        target_save_path_1=\"\",\n",
    "    ):\n",
    "        os.system(f\"mkdir -p {target_save_path_1}\")\n",
    "\n",
    "        # генерируем случайный номер для исполнителя\n",
    "        self.client_id = str(uuid.uuid4())\n",
    "\n",
    "        original_images = dataset\n",
    "        target_images = sorted(list(Path(target_save_path_1).glob(\"*.png\")))\n",
    "\n",
    "        # first time\n",
    "        if len(target_images) == 0:\n",
    "            last_frame = 0\n",
    "        else:\n",
    "            last_frame = target_images[-1]\n",
    "            last_frame = int(last_frame.stem)\n",
    "\n",
    "        print(\"last_frame\", last_frame)\n",
    "\n",
    "        if last_frame + input_frames > len(dataset):\n",
    "            return \"END\"\n",
    "\n",
    "        with open(self.workflow_path, \"r\") as f:\n",
    "            workflow = json.load(f)\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as dataset_images_dir:\n",
    "            print(f\"Temporary folder created at: {dataset_images_dir}\")\n",
    "            # for im_num in images_to_process:\n",
    "            temp_images = []\n",
    "            for im_num in range(last_frame, last_frame + input_frames):\n",
    "                save_name = \"{:07d}\".format(im_num)\n",
    "                temp_images.append(save_name)\n",
    "                dataset[int(im_num)][\"edited_image\"].save(\n",
    "                    f\"{dataset_images_dir}/{save_name}.png\"\n",
    "                )\n",
    "\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                print(f\"Temporary folder created at: {temp_dir}\")\n",
    "                # final images\n",
    "                workflow[\"277\"][\"inputs\"][\"output_path\"] = f\"{temp_dir}/\"\n",
    "                # start number\n",
    "                workflow[\"270\"][\"inputs\"][\"Number\"] = str(0)\n",
    "                workflow[\"248\"][\"inputs\"][\"image_load_cap\"] = input_frames + 1\n",
    "                workflow[\"248\"][\"inputs\"][\"folder\"] = dataset_images_dir\n",
    "                workflow[\"201\"][\"inputs\"][\"attention_mode\"] = \"sdpa\"\n",
    "\n",
    "                prompt_id = self.queue_prompt(workflow)[\"prompt_id\"]\n",
    "\n",
    "                print(self.client_id)\n",
    "                ws = websocket.WebSocket()\n",
    "                ws.connect(\n",
    "                    \"ws://{}/ws?clientId={}\".format(\n",
    "                        self.server_address,\n",
    "                        self.client_id,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                while True:\n",
    "                    out = ws.recv()\n",
    "                    if isinstance(out, str):\n",
    "                        message = json.loads(out)\n",
    "                        if message[\"type\"] == \"executing\":\n",
    "                            data = message[\"data\"]\n",
    "                            if data[\"node\"] is None and data[\"prompt_id\"] == prompt_id:\n",
    "                                break  # Execution is done\n",
    "                    else:\n",
    "                        continue  # previews are binary data\n",
    "\n",
    "                smooth_images = sorted(list(Path(temp_dir).glob(\"*.png\")))\n",
    "                print(temp_images)\n",
    "                for im, global_num in zip(smooth_images, temp_images):\n",
    "                    Image.open(im.absolute()).save(\n",
    "                        f\"{target_save_path_1}/{global_num}.png\"\n",
    "                    )\n",
    "\n",
    "\n",
    "port = 8188\n",
    "comfy_images_process = ComfyUIImageAPIUpscaleV2(\n",
    "    server_address=f\"127.0.0.1:{port}\",\n",
    "    workflow_path=\"/code/showcases/showcase_11/wan_vace_seedvr2_wan2.2_double_upscale_nfs_api_free_mem.json\",\n",
    "    # workflow_path=\"/code/showcases/showcase_11/wan_vace_seedvr2_wan2.2_double_upscale_nfs_free_mem.json\",\n",
    ")\n",
    "target_save_path = (\n",
    "    # \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_5_sdxl_1_wan\"\n",
    "    \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_6_sdxl_1_wan\"\n",
    ")\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset_name = \"dim/nfs_pix2pix_1920_1080_v5\"\n",
    "dataset_name = \"dim/nfs_pix2pix_1920_1080_v6\"\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    # cache_dir=\"/code/dataset/nfs_pix2pix_1920_1080_v5\",\n",
    "    cache_dir=\"/code/dataset/nfs_pix2pix_1920_1080_v6\",\n",
    ")\n",
    "dataset = dataset[\"train\"]\n",
    "comfy_images_process.process_image_folder(\n",
    "    dataset=dataset,\n",
    "    input_frames=250,\n",
    "    target_save_path_1=target_save_path,\n",
    ")\n",
    "# надо запускать comfy с параметром --disable-smart-memory, иначе OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_frame 248\n",
      "Temporary folder created at: /tmp/tmpn_mm1_ju\n",
      "Temporary folder created at: /tmp/tmp7_8udnxj\n",
      "48612bef-ab5f-466c-95c3-1b343fc8638a\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "\n",
    "while result != \"END\":\n",
    "    result = comfy_images_process.process_image_folder(\n",
    "        dataset=dataset,\n",
    "        input_frames=250,\n",
    "        target_save_path_1=target_save_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0b302e868d4a79aa411a0d4c07cb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "save_path_folder = (\n",
    "    \"/code/comfyui_sandbox/video_renders/render_nfs_4screens_5_sdxl_1_dataset/\"\n",
    ")\n",
    "os.system(f\"mkdir -p {save_path_folder}\")\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    dataset[i][\"edited_image\"].save(f\"{save_path_folder}{'{:07d}'.format(i)}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_code (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
