### Flow matching and diffusion
- [\[ICCV 2025\] Official Implementation of Contrastive Flow Matching](https://github.com/gstoica27/DeltaFM)
- [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://sihyun.me/REPA/)
- [RectifiedFlow is a simple, unified PyTorch codebase for diffusion and flow models](https://github.com/lqiang67/rectified-flow)
- https://www.chenyang.co/diffusion.html
- [About[ICML 2025] Official Repo for Stability-guided Adaptive Diffusion Acceleration. ðŸš€ðŸŒ™Accelerating off-the-shelf diffusion model with a unified stability criterion.](https://github.com/Ting-Justin-Jiang/sada-icml)
- [standalone_flow_matching](https://github.com/facebookresearch/flow_matching)
- [Diffusion Meets Flow Matching: Two Sides of the Same Coin](https://diffusionflow.github.io/)
- [introduction to flow matching](https://mlg.eng.cam.ac.uk/2024/01/20/flow-matching.html)
- [6.S183: A Practical Introduction to Diffusion Models IAP 2025](https://www.practical-diffusion.org/schedule/)
- [Step-by-Step Diffusion: An Elementary Tutorial](https://arxiv.org/pdf/2406.08929)

### Controlnets
- [\[NeurIPS 2025\] Universal Few-Shot Spatial Control for Diffusion Models](https://github.com/kietngt00/UFC)
- [Implicit Style-Content Separation using B-LoRA](https://b-lora.github.io/B-LoRA/)

### View Synthesis
- [\[ICCV'25\]DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion](https://github.com/wenqsun/DimensionX)

### 3D Scene Generation
- [Stable Virtual Camera: Generative View Synthesis with Diffusion Models](https://github.com/Stability-AI/stable-virtual-camera)
- [FlashWorld: High-quality 3D Scene Generation within Seconds](https://github.com/imlixinyang/FlashWorld)
- [SpatialGen: Layout-guided 3D Indoor Scene Generation](https://github.com/manycore-research/SpatialGen)
- [HunyuanWorld-Voyager](https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager)
- [HunyuanWorld-Mirror](https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror)
- [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0)
- [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://github.com/SkyworkAI/Matrix-Game)
- [Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels with Hunyuan3D World Model](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)
- [Genie 3: A new frontier for world models](https://deepmind.google/blog/genie-3-a-new-frontier-for-world-models/)


### Image Depth estimation
- [\[NeurIPS 2024\] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation](https://github.com/DepthAnything/Depth-Anything-V2)
- [DICEPTION: A Generalist Diffusion Model for Vision Perception](https://github.com/aim-uofa/Diception)
- [DepthFM: Fast Monocular Depth Estimation with Flow Matching](https://github.com/CompVis/depth-fm)
- [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://github.com/microsoft/DAViD)

### Video Edit
- [MoCha: End-to-End Video Character Replacement without Structural Guidance](https://github.com/Orange-3DV-Team/MoCha)
- 

### Video Depth estimation
- [\[CVPR 2025 Highlight\] Video Depth Anything: Consistent Depth Estimation for Super-Long Videos](https://github.com/DepthAnything/Video-Depth-Anything)
- [\[ICCV 2025\] NormalCrafter: Learning Temporally Consistent Video Normal from Video Diffusion Priors](https://github.com/Binyr/NormalCrafter)
- 

### Gaussian splatting
- [Original reference implementation of "3D Gaussian Splatting for Real-Time Radiance Field Rendering"](https://github.com/graphdeco-inria/gaussian-splatting)
- 

### Video Tracking
- [DINOv3 semantic video tracking running locally in your browser (WebGPU)](https://www.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)

### Video generators
- [ðŸŽ¬ SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://github.com/NVlabs/Sana/blob/main/asset/docs/sana_video.md)
- [Uniform Discrete Diffusion with Metric Path for Video Generation](https://bitterdhg.github.io/URSA_page/)

### Realtime
- [DiffuStreamsionV2: An Open-Source Streaming System for Real-Time Interactive Video Generation](https://github.com/chenfengxu714/StreamDiffusionV2)
- [Real-time screen-to-image generator built around StreamDiffusion.](https://github.com/rudyaa-sd/ScreenDiffusion)

### Panoramic Image Generation
- [DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training](https://fenghora.github.io/DiT360-Page/)
- 

### Video Object Segmentation
- [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://rookiexiong7.github.io/projects/SeC/)
- 

### Super-Resolution
- [Towards Real-Time Diffusion-Based Streaming Video Super-Resolution â€” An efficient one-step diffusion framework for streaming VSR with locality-constrained sparse attention and a tiny conditional decoder.](https://github.com/OpenImagingLab/FlashVSR)
- [AnyUp: Universal Feature Upsampling](https://wimmerth.github.io/anyup/)
- [The codes for Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration](https://github.com/csbhr/Vivid-VR)
- [[CVPR 2024] SinSR: Diffusion-Based Image Super-Resolution in a Single Step](https://github.com/wyf0912/SinSR)
- [[ICCV 2025] This is the official PyTorch codes for the paper: "DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution"](https://github.com/Adam-duan/DiT4SR)

### Video Vae
- [Tiny AutoEncoder for Hunyuan Video](https://github.com/madebyollin/taehv)
- 

### Comfyui
- [ComfyUI wrapper nodes for WanVideo and related models.](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ðŸ¦ŠAniSora V3 360Â° anime spins with AniSora V3.2](https://scrapbox.io/work4ai/%F0%9F%A6%8AAniSora_V3)
- [FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution,this node ,you can use it in comfyUI](https://github.com/smthemex/ComfyUI_FlashVSR)
- [2000s Analog Core - A Hi8 Camcorder LoRA for Qwen-Image](https://www.reddit.com/r/StableDiffusion/comments/1odsid9/2000s_analog_core_a_hi8_camcorder_lora_for/)
- [make the image real](https://www.reddit.com/r/StableDiffusion/comments/1naoy66/make_the_image_real/)
- [Created a Kontext LoRA that turns your phone pics into vintage film camera shots](https://www.reddit.com/r/StableDiffusion/comments/1n85ygy/created_a_kontext_lora_that_turns_your_phone_pics/)
- [KPop Demon Hunters x Friends](https://www.reddit.com/r/StableDiffusion/comments/1mx3kpd/kpop_demon_hunters_x_friends/)
- [Instagirl WAN 2.2](https://civitai.com/models/1822984?modelVersionId=2180477)
- [ComfyUI Wan2.2 Fun Native Support and LightX2V 4-Step LoRA Integration](https://blog.comfy.org/p/comfyui-wan22-fun-inp-support)
- [ComfyUI-SeedVR2_VideoUpscaler](https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler)
- [Qwen-Image Image Structure Control Model](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny/summary)
- [https://huggingface.co/Qwen/Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)
- [Generating Multiple Views from One Image Using Flux Kontext in ComfyUI](https://www.reddit.com/r/comfyui/comments/1mixszv/generating_multiple_views_from_one_image_using/)
- [Wan-AI/Wan2.2-Animate-14B](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B)
- [UltraReal + Nice Girls LoRAs for Qwen-Image](https://www.reddit.com/r/StableDiffusion/comments/1mnlw3l/ultrareal_nice_girls_loras_for_qwenimage/)
- [Qwen Image Models Training - 0 to Hero Level Tutorial - LoRA & Fine Tuning - Base & Edit Model](https://www.youtube.com/watch?v=DPX3eBTuO_Y)
- [How to make 3D/2.5D images look more realistic?](https://www.reddit.com/r/comfyui/comments/1oi1yu2/how_to_make_3d25d_images_look_more_realistic/)


### Wan model
- [Wan Video Ditto in ComfyUI - Transform ANY Video with Just a Text Prompt!](https://youtu.be/yvIubLbSJ-g?si=r410CAHe4KjcrOBK)